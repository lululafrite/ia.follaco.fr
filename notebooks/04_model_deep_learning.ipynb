{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38364d87",
   "metadata": {},
   "source": [
    "# Notebook `04_model_deep_learning.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d423b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb1b615",
   "metadata": {},
   "source": [
    "# üß† Construction et Entra√Ænement du Mod√®le Deep Learning\n",
    "\n",
    "Ce notebook constitue la **quatri√®me √©tape du pipeline IA**.  \n",
    "Il est d√©di√© √† la construction, l‚Äôentra√Ænement, l‚Äô√©valuation et la sauvegarde d‚Äôun **mod√®le de deep learning** destin√© √† pr√©dire le **prix d‚Äôun service Fiverr** √† partir de variables num√©riques et vectorielles.\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "- Charger les donn√©es propres et transform√©es (`fiverr_cleaned_transformed.csv`)\n",
    "- G√©n√©rer les **embeddings vectoriels** √† partir des descriptions (mod√®le `SentenceTransformer`)\n",
    "- Pr√©parer les **entr√©es combin√©es** : texte vectoris√©, niveau encod√©, fiabilit√© num√©rique\n",
    "- D√©finir un **mod√®le Keras s√©quentiel** adapt√© √† la r√©gression\n",
    "- R√©aliser une **s√©paration train/test** et standardiser les variables\n",
    "- Entra√Æner le mod√®le avec **early stopping** pour √©viter le surapprentissage\n",
    "- √âvaluer ses performances sur les donn√©es de test\n",
    "- Sauvegarder le mod√®le (`deep_model.h5`) et le scaler (`scaler.pkl`)\n",
    "\n",
    "## ‚úÖ Comp√©tences mobilis√©es\n",
    "\n",
    "- **Bloc 3 ‚Äî C3** : Impl√©menter un mod√®le de deep learning adapt√© √† un jeu de donn√©es structur√©\n",
    "- **Bloc 3 ‚Äî C2** : Pr√©parer les donn√©es et normaliser les vecteurs d‚Äôentr√©e (embedding + features classiques)\n",
    "- **Bloc 5 ‚Äî C4** : Exporter un mod√®le exploitable dans un environnement d√©ploy√© (API, Gradio)\n",
    "\n",
    "*Ce notebook pr√©pare un mod√®le de pr√©diction avanc√© bas√© sur les r√©seaux de neurones, utilis√© dans l‚Äôapplication finale.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61762ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bafbc79",
   "metadata": {},
   "source": [
    "## üß≠ Sommaire\n",
    "\n",
    "1. [Importation des biblioth√®ques pour le Deep Learning](#-1-importation-des-biblioth√®ques-pour-le-deep-learning)\n",
    "2. [Chargement des donn√©es transform√©es dans un DataFrame Pandas](#-2-chargement-des-donn√©es-transform√©es-dans-un-dataframe-pandas)\n",
    "3. [Pr√©paration des donn√©es pour le Deep Learning](#-3-pr√©paration-des-donn√©es-pour-le-deep-learning)\n",
    "4. [Construction du mod√®le MLP (r√©gression du prix)](#-4-construction-du-mod√®le-mlp-r√©gression-du-prix)\n",
    "5. [Entra√Ænement du mod√®le avec EarlyStopping](#-5-entra√Ænement-du-mod√®le-avec-earlystopping)\n",
    "6. [√âvaluation du mod√®le entra√Æn√©](#-6-√©valuation-du-mod√®le-entra√Æn√©)\n",
    "7. [Sauvegarde du mod√®le Deep Learning entra√Æn√©](#-7-sauvegarde-du-mod√®le-deep-learning-entra√Æn√©)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14adee5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46169b50",
   "metadata": {},
   "source": [
    "## üß† 1. Importation des biblioth√®ques pour le Deep Learning\n",
    "\n",
    "### ‚ùì 1.1. Pourquoi cette √©tape maintenant ?\n",
    "\n",
    "Avant toute manipulation ou entra√Ænement, nous devons importer toutes les **biblioth√®ques n√©cessaires** √† la gestion des donn√©es, √† la construction du mod√®le, et √† l‚Äôing√©nierie des variables.\n",
    "\n",
    "Cela garantit :\n",
    "- un environnement pr√™t √† ex√©cuter le pipeline complet,\n",
    "- une meilleure lisibilit√© du script,\n",
    "- et la centralisation des d√©pendances en d√©but de fichier.\n",
    "\n",
    "### üéØ 1.2. R√©sultat attendu\n",
    "\n",
    "- Toutes les librairies utiles au traitement et √† l'entra√Ænement d‚Äôun mod√®le Deep Learning sont import√©es.\n",
    "- L'importation est **clairement organis√©e** par type de t√¢che (donn√©es, mod√®le, I/O, etc.).\n",
    "- Aucune erreur d'importation ne bloque l'ex√©cution du notebook.\n",
    "\n",
    "---\n",
    "\n",
    "### üêç 1.3. Script d‚Äôimportation des biblioth√®ques n√©cessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f125f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des biblioth√®ques n√©cessaires\n",
    "\n",
    "# Manipulation de donn√©es\n",
    "import pandas as pd   # Biblioth√®que pour la gestion des tableaux de donn√©es (DataFrame)\n",
    "import numpy as np    # Biblioth√®que pour le calcul num√©rique performant (vecteurs, matrices, etc.)\n",
    "\n",
    "# Deep Learning avec TensorFlow Keras\n",
    "import tensorflow as tf  # Backend TensorFlow (n√©cessaire m√™me si Keras est utilis√© seul)\n",
    "from tensorflow.keras.models import Sequential       # Mod√®le lin√©aire empil√© (s√©quentiel)\n",
    "from tensorflow.keras.layers import Dense, Dropout   # Couches dense (fully connected) et dropout (r√©gularisation)\n",
    "from tensorflow.keras.callbacks import EarlyStopping # Callback pour arr√™ter l'entra√Ænement en cas de surapprentissage\n",
    "\n",
    "# Pr√©paration et √©valuation des donn√©es\n",
    "from sklearn.model_selection import train_test_split  # Fonction de s√©paration du dataset en ensembles d'entra√Ænement/test\n",
    "from sklearn.preprocessing import StandardScaler       # Standardisation (centrage/r√©duction) des variables num√©riques\n",
    "\n",
    "# Gestion des fichiers et mod√®les\n",
    "import os       # Outils de gestion de fichiers et r√©pertoires\n",
    "import joblib   # Sauvegarde et chargement efficace des objets Python (mod√®les, scalers, etc.)\n",
    "\n",
    "# Embedding de texte via transformers\n",
    "from sentence_transformers import SentenceTransformer  # G√©n√©ration d‚Äôembeddings vectoriels √† partir de textes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be1724",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2cb97",
   "metadata": {},
   "source": [
    "## üìÇ 2. Chargement des donn√©es transform√©es dans un DataFrame Pandas\n",
    "\n",
    "### ‚ùì 2.1. Pourquoi cette √©tape maintenant ?\n",
    "\n",
    "Le fichier `fiverr_cleaned_transformed.csv` contient les donn√©es **nettoy√©es et enrichies** suite aux √©tapes de pr√©traitement pr√©c√©dentes.  \n",
    "C‚Äôest √† partir de ce fichier que nous allons **pr√©parer les entr√©es** du mod√®le de deep learning.\n",
    "\n",
    "Cette √©tape permet :\n",
    "- de **valider l‚Äôacc√®s au fichier** et le bon format CSV,\n",
    "- d‚Äôinitialiser le DataFrame `df` pour les traitements ult√©rieurs,\n",
    "- d‚Äôobtenir une **confirmation imm√©diate** sur le nombre de lignes et colonnes disponibles.\n",
    "\n",
    "### üéØ 2.2. R√©sultat attendu\n",
    "\n",
    "- Les donn√©es sont correctement lues dans le DataFrame `df`.\n",
    "- Aucune erreur d‚Äôacc√®s ou de lecture n‚Äôest rencontr√©e.\n",
    "- Le terminal affiche les dimensions des donn√©es (nombre de lignes et de colonnes).\n",
    "\n",
    "---\n",
    "\n",
    "### üêç 2.3. Script de chargement des donn√©es transform√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8395a1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donn√©es charg√©es : (1145, 5)\n"
     ]
    }
   ],
   "source": [
    "# üîπ Chargement des donn√©es\n",
    "\n",
    "# D√©finition du chemin vers le fichier CSV nettoy√© et transform√©\n",
    "file_path = \"../data/fiverr_cleaned_dl_notebook.csv\"\n",
    "\n",
    "# Chargement du fichier CSV dans un DataFrame Pandas\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Affichage d'un message de confirmation avec les dimensions des donn√©es charg√©es\n",
    "print(\"Donn√©es charg√©es :\", df.shape)  # Exemple : (25000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d675bd90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96734c6",
   "metadata": {},
   "source": [
    "## üß™ 3. Pr√©paration des donn√©es pour le Deep Learning\n",
    "\n",
    "### ‚ùì 3.1. Pourquoi cette √©tape maintenant ?\n",
    "\n",
    "Cette √©tape pr√©pare les **entr√©es du mod√®le de deep learning** :\n",
    "- G√©n√©ration des **embeddings vectoriels** pour les descriptions textuelles,\n",
    "- **Encodage one-hot** de la variable cat√©gorielle `Niveau`,\n",
    "- S√©lection et concat√©nation des **variables num√©riques** comme `Fiabilite`,\n",
    "- S√©paration du jeu de donn√©es en ensembles d‚Äôentra√Ænement et de test,\n",
    "- **Normalisation des features** pour stabiliser l‚Äôapprentissage,\n",
    "- Sauvegarde du scaler pour reproduire le pipeline d‚Äôinf√©rence plus tard.\n",
    "\n",
    "C‚Äôest une √©tape centrale avant toute mod√©lisation supervis√©e.\n",
    "\n",
    "### üéØ 3.2. R√©sultat attendu\n",
    "\n",
    "- Le DataFrame `X` contient toutes les variables explicatives correctement format√©es.\n",
    "- Les jeux `X_train_scaled`, `X_test_scaled`, `y_train`, `y_test` sont pr√™ts √† l‚Äôusage.\n",
    "- Le scaler `StandardScaler` est sauvegard√© dans `models/deep/scaler.pkl`.\n",
    "- Les dimensions du jeu d‚Äôentra√Ænement sont affich√©es √† l‚Äô√©cran pour validation.\n",
    "\n",
    "---\n",
    "\n",
    "### üêç 3.3. Script de pr√©paration des features d'entr√©e pour le deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d002c75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler sauvegard√© : ../models/deep/scaler_notebook.pkl\n",
      "Donn√©es pr√™tes pour entra√Ænement deep learning : (916, 388)\n"
     ]
    }
   ],
   "source": [
    "# Chargement des donn√©es\n",
    "df = pd.read_csv(file_path)  # Lecture du fichier transform√© contenant les features pr√™tes √† l‚Äôemploi\n",
    "\n",
    "# Chargement du mod√®le d'embedding SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # Mod√®le l√©ger et performant pour vectoriser les descriptions textuelles\n",
    "\n",
    "# Embedding de la colonne 'Description'\n",
    "descriptions = df[\"Description\"].astype(str).tolist()\n",
    "embeddings = embedding_model.encode(descriptions)\n",
    "embed_df = pd.DataFrame(embeddings, columns=[f\"emb_{i}\" for i in range(embeddings.shape[1])])  # Cr√©ation d‚Äôun DataFrame pour les vecteurs d‚Äôembedding\n",
    "\n",
    "# Encodage one-hot du niveau du vendeur\n",
    "niveau_encoded = pd.get_dummies(df[\"Niveau\"], prefix=\"Niveau\")  # Conversion de la variable cat√©gorielle en variables binaires\n",
    "\n",
    "# S√©lection des variables num√©riques restantes\n",
    "autres_features = df[[\"Fiabilite\"]].reset_index(drop=True)  # Ajout de la variable num√©rique \"Fiabilite\"\n",
    "\n",
    "# Fusion finale des features dans X\n",
    "X = pd.concat([embed_df, niveau_encoded, autres_features], axis=1)  # Construction du tableau final de variables explicatives\n",
    "y = df[\"Prix\"]  # Variable cible : le prix\n",
    "\n",
    "# D√©coupage du jeu de donn√©es en train / test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% entra√Ænement, 20% test\n",
    "\n",
    "# Standardisation des donn√©es (centrage-r√©duction)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Apprentissage des param√®tres sur X_train\n",
    "X_test_scaled = scaler.transform(X_test)        # Transformation de X_test avec les m√™mes param√®tres\n",
    "\n",
    "# Sauvegarde du scaler pour une r√©utilisation future\n",
    "os.makedirs(\"../models/deep\", exist_ok=True)\n",
    "scaler_path = \"../models/deep/scaler_notebook.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "# Messages de v√©rification\n",
    "print(\"Scaler sauvegard√© :\", scaler_path)\n",
    "print(\"Donn√©es pr√™tes pour entra√Ænement deep learning :\", X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15496d22",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97721ffe",
   "metadata": {},
   "source": [
    "## üß† 4. Construction du mod√®le MLP (r√©gression du prix)\n",
    "\n",
    "### ‚ùì 4.1. Pourquoi cette √©tape maintenant ?\n",
    "\n",
    "Nous allons entra√Æner un mod√®le de Deep Learning de type **MLP (Multilayer Perceptron)** pour pr√©dire le prix d‚Äôun service.  \n",
    "Il s‚Äôagit d‚Äôun mod√®le dense √† plusieurs couches, adapt√© aux jeux de donn√©es tabulaires enrichis (num√©riques + embeddings).\n",
    "\n",
    "Le mod√®le est con√ßu pour apprendre une **fonction de r√©gression** sur les variables d‚Äôentr√©e (dont les embeddings de description) vers une **valeur continue de prix**.\n",
    "\n",
    "### üéØ 4.2. R√©sultat attendu\n",
    "\n",
    "- Un mod√®le Keras `Sequential` est initialis√© avec 3 couches :\n",
    "  - Deux couches cach√©es avec activations ReLU.\n",
    "  - Une couche de sortie sans activation (r√©gression directe).\n",
    "- Une couche de r√©gularisation `Dropout` est int√©gr√©e pour r√©duire le risque de surapprentissage.\n",
    "- Le mod√®le est compil√© avec :\n",
    "  - L‚Äôoptimiseur `adam`\n",
    "  - La fonction de perte `mse`\n",
    "  - L‚Äôindicateur de performance `mae`\n",
    "\n",
    "---\n",
    "\n",
    "### üêç 4.3. Script de d√©finition du mod√®le MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45cab3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du mod√®le MLP (Multilayer Perceptron)\n",
    "\n",
    "# Initialisation d'un mod√®le s√©quentiel Keras\n",
    "model = Sequential([\n",
    "\n",
    "    # Premi√®re couche cach√©e dense avec 128 neurones et une activation ReLU\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "\n",
    "    # Couche de dropout pour limiter le surapprentissage (20% des neurones d√©sactiv√©s √† chaque it√©ration)\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Deuxi√®me couche cach√©e dense avec 64 neurones et une activation ReLU\n",
    "    Dense(64, activation='relu'),\n",
    "\n",
    "    # Couche de sortie : une seule valeur continue (r√©gression du prix)\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compilation du mod√®le avec :\n",
    "# - l'optimiseur 'adam' (rapide et efficace pour la majorit√© des cas)\n",
    "# - la fonction de perte 'mse' (erreur quadratique moyenne, adapt√©e √† la r√©gression)\n",
    "# - l'indicateur de performance 'mae' (erreur absolue moyenne, plus lisible pour l'utilisateur final)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee123e54",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558e9a5",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÇÔ∏è 5. Entra√Ænement du mod√®le avec EarlyStopping\n",
    "\n",
    "### ‚ùì 5.1. Pourquoi cette √©tape maintenant ?\n",
    "\n",
    "Apr√®s avoir pr√©par√© les donn√©es et construit notre mod√®le de deep learning, il est temps de lancer l‚Äôentra√Ænement.  \n",
    "Nous utilisons ici un m√©canisme de **surveillance automatique** pour √©viter le surapprentissage (`EarlyStopping`).\n",
    "\n",
    "Ce m√©canisme permet :\n",
    "- d'interrompre l'entra√Ænement si le mod√®le ne s'am√©liore plus sur les donn√©es de validation,\n",
    "- d'√©viter d‚Äôapprendre des d√©tails trop sp√©cifiques √† l‚Äô√©chantillon d‚Äôentra√Ænement (overfitting),\n",
    "- de restaurer automatiquement les **meilleurs poids** enregistr√©s.\n",
    "\n",
    "### üéØ 5.2. R√©sultat attendu\n",
    "\n",
    "- Le mod√®le est entra√Æn√© sur les donn√©es normalis√©es `X_train_scaled`.\n",
    "- Une **validation crois√©e interne** est effectu√©e √† chaque √©poque sur 20% des donn√©es.\n",
    "- Le processus s‚Äôinterrompt automatiquement si aucune am√©lioration n‚Äôest constat√©e pendant 10 √©poques cons√©cutives.\n",
    "- L‚Äôobjet `history` contient toutes les informations n√©cessaires √† la visualisation de la courbe d‚Äôapprentissage.\n",
    "\n",
    "---\n",
    "\n",
    "### üêç 5.3. Script d‚Äôentra√Ænement avec arr√™t anticip√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b30b7dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - 1s 14ms/step - loss: 36.9353 - mae: 4.5490 - val_loss: 34.3361 - val_mae: 4.6683\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0065 - mae: 3.8876 - val_loss: 35.1867 - val_mae: 4.6080\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.3895 - mae: 3.6078 - val_loss: 34.1707 - val_mae: 4.6207\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.3212 - mae: 3.3273 - val_loss: 34.8686 - val_mae: 4.5734\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 16.5729 - mae: 3.1223 - val_loss: 34.3977 - val_mae: 4.5513\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 14.3335 - mae: 2.8877 - val_loss: 33.6412 - val_mae: 4.4827\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.6017 - mae: 2.6869 - val_loss: 34.6260 - val_mae: 4.5464\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 11.7556 - mae: 2.5521 - val_loss: 34.9099 - val_mae: 4.5528\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 10.4761 - mae: 2.3884 - val_loss: 35.9312 - val_mae: 4.5713\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 9.0381 - mae: 2.2173 - val_loss: 35.4424 - val_mae: 4.4865\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.2618 - mae: 2.1110 - val_loss: 35.4345 - val_mae: 4.5426\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 7.6078 - mae: 1.9956 - val_loss: 35.1565 - val_mae: 4.5188\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.8398 - mae: 1.8995 - val_loss: 36.3944 - val_mae: 4.5389\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 6.5078 - mae: 1.9170 - val_loss: 36.0569 - val_mae: 4.5204\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 6.4687 - mae: 1.8712 - val_loss: 35.8081 - val_mae: 4.4748\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 6.2215 - mae: 1.8142 - val_loss: 36.1844 - val_mae: 4.4898\n"
     ]
    }
   ],
   "source": [
    "# Entra√Ænement avec early stopping\n",
    "\n",
    "# Cr√©ation d‚Äôun callback EarlyStopping :\n",
    "# - 'monitor' : indique que l'on surveille la perte sur les donn√©es de validation ('val_loss').\n",
    "# - 'patience' : arr√™te l'entra√Ænement si la perte ne s'am√©liore pas apr√®s 10 epochs cons√©cutifs.\n",
    "# - 'restore_best_weights' : restaure les poids du mod√®le obtenus √† l'√©poque avec la meilleure val_loss.\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Entra√Ænement du mod√®le sur les donn√©es d‚Äôentra√Ænement\n",
    "# - 'validation_split' : 20% des donn√©es d'entra√Ænement sont utilis√©es pour valider le mod√®le pendant l'entra√Ænement.\n",
    "# - 'epochs' : nombre maximum d'it√©rations (epochs).\n",
    "# - 'batch_size' : nombre d'exemples trait√©s avant la mise √† jour des poids.\n",
    "# - 'callbacks' : utilise le m√©canisme d'arr√™t anticip√© pour √©viter l‚Äôoverfitting.\n",
    "# - 'verbose' : 1 pour affichage d√©taill√© de la progression dans le terminal.\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c27b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d1cfd4",
   "metadata": {},
   "source": [
    "## üß™ 6. √âvaluation du mod√®le entra√Æn√©\n",
    "\n",
    "### ‚ùì 6.1. Pourquoi cette √©tape maintenant ?\n",
    "\n",
    "L‚Äô√©valuation finale permet de mesurer la **performance r√©elle** du mod√®le sur des donn√©es **in√©dites** (non vues pendant l‚Äôentra√Ænement).  \n",
    "Cela permet de d√©tecter :\n",
    "- Un √©ventuel **surapprentissage** si la performance chute trop par rapport au jeu d‚Äôentra√Ænement,\n",
    "- L‚Äôefficacit√© globale du mod√®le dans un contexte d‚Äôusage r√©el.\n",
    "\n",
    "### üéØ 6.2. R√©sultat attendu\n",
    "\n",
    "- Le mod√®le retourne deux indicateurs cl√©s :\n",
    "  - **MAE** (*Mean Absolute Error*) : √©cart moyen absolu entre les prix r√©els et pr√©dits,\n",
    "  - **MSE** (*Mean Squared Error*) : utilis√© comme fonction de perte pour l‚Äôentra√Ænement.\n",
    "- Ces valeurs sont imprim√©es dans la console pour analyse comparative.\n",
    "\n",
    "---\n",
    "\n",
    "### üêç 6.3. Script d‚Äô√©valuation du mod√®le sur le jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d6e6722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step - loss: 29.3919 - mae: 3.9796\n",
      "\n",
      "√âvaluation finale - MAE : 3.98, MSE : 29.39\n"
     ]
    }
   ],
   "source": [
    "# √âvaluation du mod√®le sur les donn√©es de test\n",
    "\n",
    "# √âvaluation finale du mod√®le entra√Æn√© √† l'aide des donn√©es de test standardis√©es.\n",
    "# La m√©thode 'evaluate' retourne deux m√©triques :\n",
    "# - loss : ici c‚Äôest le MSE (Mean Squared Error) car le mod√®le a √©t√© compil√© avec la perte \"mse\"\n",
    "# - mae : Mean Absolute Error, plus lisible et moins sensible aux grandes erreurs\n",
    "loss, mae = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "# Affichage des r√©sultats arrondis √† deux d√©cimales\n",
    "print(f\"\\n√âvaluation finale - MAE : {mae:.2f}, MSE : {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb93841",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d563f",
   "metadata": {},
   "source": [
    "## üíæ 7. Sauvegarde du mod√®le Deep Learning entra√Æn√©\n",
    "\n",
    "### ‚ùì 7.1. Pourquoi cette √©tape maintenant ?\n",
    "\n",
    "Une fois le mod√®le entra√Æn√© et valid√©, il est crucial de **le sauvegarder** afin de :\n",
    "- R√©utiliser le mod√®le plus tard sans avoir √† le r√©entra√Æner,\n",
    "- L‚Äôint√©grer dans une application (API, Gradio, etc.),\n",
    "- Conserver une version stable du mod√®le pour reproductibilit√© ou archivage.\n",
    "\n",
    "Le format `.h5` est un format standard de sauvegarde pour les mod√®les Keras.\n",
    "\n",
    "### üéØ 7.2. R√©sultat attendu\n",
    "\n",
    "- Le mod√®le est sauvegard√© dans le fichier `models/deep/deep_model.h5`.\n",
    "- Un message de confirmation s‚Äôaffiche dans le terminal.\n",
    "\n",
    "---\n",
    "\n",
    "### üêç 7.3. Script de sauvegarde du mod√®le Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9086da16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod√®le sauvegard√© : ../models/deep/deep_model_notebook.h5\n"
     ]
    }
   ],
   "source": [
    "# üîπ Sauvegarde du mod√®le Keras\n",
    "\n",
    "# D√©finition du chemin de sauvegarde du mod√®le Deep Learning\n",
    "model_path = \"../models/deep/deep_model_notebook.h5\"\n",
    "\n",
    "# Sauvegarde du mod√®le Keras au format HDF5 (.h5)\n",
    "model.save(model_path)\n",
    "\n",
    "# Confirmation de la sauvegarde\n",
    "print(\"Mod√®le sauvegard√© :\", model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
